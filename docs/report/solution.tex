\chapter{Solution}
\label{ch:Model}

% We [outline|describe|introduce] our [proposed] [solution|model], [justify] design decisions and [implementation details| detail implementation| document implementation details].
In this chapter, we describe our experiments, justify design decisions and detail our implementation. A high-level overview of our solution is provided in Fig.~\ref{fig:overview}. 
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{plots/overview.pdf}
	\caption[Overview of the solution]{Overview of our solution. We divide the database into training and test patients, train a convolutional network with the training set (we select hyperparameters $\alpha$ and $\lambda$ using 20\% of the training set for validation) and evaluate our model in the test set. We repeat each experiment for five folds. Experiments test different preprocessing and network configurations.}
	\label{fig:overview}
\end{figure}

\section{Task definition}
We segment digital mammograms into two separate regions: breast mass (benign or malignant) and general tissue.
Breast area is previously separated from the background by simple thresholding.
In particular, we train a convolutional network to estimate the probability of each pixel belonging to a mass and evaluate these predictions.

\section{Data set}
\input{solution/dataset}

\section{Model 1}
\label{sec:Model1}
\input{solution/model1}

\section{Model 2}
\label{sec:Model 2}
\input{solution/model2}

\section{Model 3}
\label{sec:Model3}
\input{solution/model3}

\section{Training}
\input{solution/training}

\section{Evaluation}
\input{solution/evaluation}

\section{Summary}
Mammograms from the BCDR-D01 database were enhanced, resized and divided to obtain our data set. A simple architecture (6 layers, 206K parameters) was used for the first experiments, one of which used a weighted loss function to tackle class imbalance. Two more sophisticated architectures based on the VGG network (9 layers, 2.9M parameters) and residual networks (10 layers, 0.9M parameters) were used for the following experiments. We performed hyperparameter search to fine tune the learning rate and regularization parameter of each network; other hyperparameters were set manually. Networks were written in TensorFlow and optimized using ADAM. We use five-fold cross-validation for evaluation. We report the FROC curve for the final models.
