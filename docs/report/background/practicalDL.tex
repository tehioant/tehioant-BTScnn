In this section we collect guidelines for building as well as efficiently training deep convolutional networks; while they are specific to this thesis, they may prove useful in similar projects. Deep learning is a fast-changing field so these recommendations may soon outdate.
% Some of these details are taken care by the software(either as a defualt or optional feature) and some are quite new and need to be taken care manually. 

\paragraph{Image preprocessing} Convolutional networks handle raw image data, but some level of preprocessing speeds training and improves performance.
\begin{itemize}
	\item Crop images to contain only the relevant regions, denoise, enhance and resize them to maintain the input size fixed and manageable.

	\item Zero-center each image feature (the raw pixels) by subtracting its mean across all training images. Optionally, normalize each zero-centered feature to range from $[-1 \dots 1]$ by dividing them by its standard deviation~\cite{Karpathy2016}.

	\item Test data should not be used to calculate any statistic used for preprocessing. Furthermore, the same statistics (calculated from training data) should be used to preprocess test data~\cite{Karpathy2016}.
\end{itemize}

\paragraph{Convolutional network architecture} Designing convolutional networks involves many decisions, we provide recommendations for the architecture and sensible values for related hyperparameters.

\begin{itemize}
	\item Select a network architecture flexible enough to model the data and manage overfitting rather than a simpler architecture that may be incapable to model the data~\cite{Ng2014, Krizhevsky2012}. 

	\item Although, theoretically, neural networks with a single hidden layer are universal approximators provided they have enough units ($\mathcal{O}(2^n)$ where $n$ is the size of the input), practically, deeper architectures produce better results using less units overall. This holds for convolutional networks~\cite{Bengio2014}.

	\item Use at least 8 layers (not counting pooling or ReLU layers) for big data sets, use less layers or transfer learning for small data sets. ``You should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.''~\cite{Karpathy2016}

	\item Use 2-5 \texttt{CONV -> RELU} pairs before pooling (N above)~\cite{Karpathy2016}. Pooling is a destructive operation, placing two convolutional layers together allows them to detect more complex features.

	\item Use 1-8 \texttt{[CONV -> RELU]+ -> POOL} blocks (M above). This hyperparameter regulates the representational power of the architecture. The exact number depends on the complexity of the features in the data and the computational resources available. It also defines how much the volume is subsampled.

	\item Use less than 3 \texttt{FC -> RELU} pairs before the output layer (K above)~\cite{Karpathy2016}. The volume that arrives to fully connected layers is already complex, adding many layers only increases the number of parameters and risks overfitting.

	\item The number of filters per convolutional layer controls the number of features detected at that layer---similar to the number of units per layer in a regular neural network. A common pattern is to start with a small amount of filters and increase them layer by layer~\cite{Simonyan2014}. %The reasoning is that at higher layers there are more complex features to learn and moreover as the feature maps become smaller (via pooling) it is computationally feasible to have more of them.

	\item The number of filters per fully connected layer decreases layer by layer\footnote{The number of units in a convolutional layer is the number of units in a feature map times the number of feature maps.}. For instance, for a convolutional network with ten possible classes and two fully connected layers, if the last convolutional layer produces a volume of size $8 \times 8 \times 512$ (8192 units), the first fully connected layer could have size $1 \times 1 \times 2048$ and the second--- the output---layer $1\times 1\times 10$.

	\item Use $3\times 3$ filters with stride 1 and zero-padding 1 or $5 \times 5$ filters with stride 1 and zero-padding 2. This preserves the spatial dimensions of the volume and works better in practice~\cite{Springenberg2014}. If the input size is too big, use a bigger filter in the first convolutional layer~\cite{Karpathy2016}.
	
	\item Use $2\times2$ pooling with stride 2. Overlapping max pooling may produce slightly better results but slows training and causes resizing headaches~\cite{Krizhevsky2012, Dieleman2015}.
%krizhevsky says overlapping is slightly better. so does dieleman but it slows thing down.
	\item Use square input images (width = height) with spatial dimensions divisible by 2 at least as many times as the number of pooling layers in the network.

	\item Use number of parameters to measure the complexity of an architecture rather than number of layers or units.
\end{itemize}



\paragraph{Hyperparameters} About setting and searching hyperparameters other than those of the network architecture.

\begin{itemize}
	\item Use a single sufficiently large validation set (15-30\% of data) rather than cross validation~\cite{Bengio2014}. Use cross validation in very small data sets~\cite{Ng2014}.

	\item Use random search rather than grid search. Random search draws each parameter from a value distribution rather than from a set of predefined values.~\cite{Bergstra2012}

	\item Search for the best combination of hyperparameters rather than each individually.

	\item Train each combination of hyperparameters for 1-2 epochs to narrow the search space; then, train for more epochs on these ranges~\cite{Karpathy2016}. Explore further when the best value for a hyperparameter is found in the limit of the range.~\cite{Bengio2012}.

	\item Partial convergence is sufficient to assess hyperparameters~\cite{Karpathy2016}.

	\item Hyperparameters related to the convolutional architecture, e.g., number of layers, number of filters and filter sizes are set manually (as explained above) rather than using a validation set.

	\item Several hyperparameters are set: initial learning rate $\alpha$, learning rate decay schedule, regularization strength $\lambda$, Adam hyperparameters ($\beta_1$, $\beta_2$ and $\epsilon$), probability of keeping a unit active in dropout $p$, mini-batch size and type of image preprocessing.

	\item We could fit all hyperparameters using a validation set but, in practice, this is computationally unfeasible and results in overfitting to the validation data~\cite{Cawley2010}.

	\item Set $\alpha$, $\lambda$ and optionally the type of preprocessing using a validation set. Other hyperparameters can be set to a sensible default. 
%The learning rate schedule and training epochs are set using heuristics. 

	\item The learning rate $\alpha$ is ``the single most important hyperparameter and one should always make sure that it has been tuned''~\cite{Bengio2012}. It ranges from $10^{-6}$ to $10^{0}$. Use a log scale to draw new values ($\alpha = 10^{unif(-6, 0)}$ where $unif(a,b)$ is the continous uniform distribution)~\cite{Karpathy2016}.
%0.01: Bengio, 2012 says that the optimal learning rate is close to the highest learning rate that does not cause divergence.
% If your loss doen't decrease, learning rate is too low, if it explodes, too high.

	\item The regularization strength $\lambda$ is usually data (and loss function) dependant. It ranges from $10^{-3}$ to $10^4$. Search in log scale ($\lambda = 10^{unif(-3, 4)}$).
%1
	\item Divide the learning rate by 10 every time the validation error stops improving or every fixed number of epochs chosen by observing when it stops improving in a similar network~\cite{Krizhevsky2012, He2015b}.

	%\item Use $\mu=0.9$. If using a validation set try values in \{0.5, 0.9, 0.95, 0.99\}~\cite{Karpathy2016}.% For NAG.

	\item Use $\beta_1 = 0.9$, $\beta_2 = 0.995$ and $\epsilon = 1 {\times} 10^{-6}$~\cite{Karpathy2016}.
	% Tensorflow says epsilon of 1e-8 is way too small. Proposes up to epsilon = 1 but that may be only for big networks.

	\item Use 0.9-1 probability $p$ of retaining a unit in the input layer, 0.65-0.85 in the first 2-4 convolutional layers and 0.5 in the last convolutional layers and all fully connected layers~\cite{Srivastava2014}. Less dropout is used on the first layers because they have less parameters~\cite{Karpathy2016}.
% Don't use dropout in the input layer, because putting a zero there has a meaning(black), maybe the advantage of dropout in cinvolutional layers is just that it adds noise to the input.

	\item Use mini-batch size of 64 or 32. A larger batch size requires more memory and training time. Test performance is unaffected~\cite{Bengio2012}.

	\item Choose among standard preprocessing techniques by (qualitatively) inspecting results on images from the validation set. If none seems superior, fit it along $\alpha$ and $\lambda$% other hyperparameters.
\end{itemize}


\paragraph{Training} Convolutional networks have millions of parameters and require careful training. We offer general advice and list some commonly used techniques.

\begin{itemize}
	%\item Store your images as binary hdf5 files if big data. This allows fster fetcjhing during training.If small data sets, JSON is fine~cite{Karpathy2016}
% https://github.com/jcjohnson/torch-rnn/blob/master/scripts/preprocess.py

	\item Shuffle training examples before each epoch to ensure that examples in every mini-batch are sampled independently~\cite{Bengio2012}.

	\item Multiply the number of examples by 25-100 to estimate the maximum number of parameters that could be learned (assuming data augmentation). Groups have learned up to 40M parameters from as little as 60K training examples~\cite{Dieleman2015, Springenberg2014}.

	\item Weight initialization is vital for convergence. Initialize each filter weight as a value from a normal distribution $\mathcal{N}(\mu = 0, \sigma = \sqrt{2/n_{in}})$ where $n_{in}$ is the number of connections to the filter (90 for a $3\times 3$ filter with depth $10$)~\cite{He2015a}. In code, \texttt{w = randn()*sqrt(2/nIn)} where \texttt{randn()} returns values drawn from a standard normal distribution. Initialize biases to zero.

	\item Use mini-batches rather than the entire training set to compute the gradient of the loss function. Mini-batches allows us to make more updates, more frequently resulting in faster convergence and better test results~\cite{Bengio2012}.

	\item Use (inverted) dropout~\cite{Krizhevsky2012} as a complement to $l_2$-norm regularization. Dropout improves results but may slow network convergence.

	\item Batch normalize the output of each convolutional layer~\cite{Ioffe2015}. This speeds convergence (using higher learning rates and faster decay) and reduces the need for dropout. %It learns two extra parameters per feature map.
	%If my batch is a single image it will normalize over each feature map (mini-batch =1), not sure this is what is intended to do/will help. Although, it could learn to go back to normal. They only use mini-batches of size 32.

	\item Use leaky rectified linear units (\texttt{max(0.1x, x)}) rather than simple rectified linear units. Leaky ReLUs slightly improve results at almost no cost~\cite{Xu2015}.

	\item Use Adam~\cite{Kingma2014} to update weights. Adam incoporates momentum and per-parameter adaptive learning rates. Nesterov's Accelerated Gradient is also a viable option~\cite{Karpathy2016}.
	%\item Use Nesterov's Accelerated Gradient (NAG) to update the weights. NAG is a modified version of gradient descent that works better for certain architectures~\cite{Bengio2012b}.

	\item Store network parameters regularly during training. This allows us to inspect the network at different stages, retrain one or select the one with the best validation error~\cite{Bengio2014}.
	%\item Use the validation or test error to select the best parameters for the network from those stored~\cite{Bengio2014}.

	\item Stop training if the validation error has not improved since the last learning rate reduction: gradient descent may not have converged but the validation error will start to increase (overfit)~\cite{Bengio2012}.

	\item If you used test set results to refine a model, shuffle the entire data set and choose a different training and test set to avoid overfitting to the test set~\cite{Ng2014}.
\end{itemize}


\paragraph{Sanity checks}
We perform some simple tests to make sure training works properly.
\begin{itemize}
	\item After weight initialization, run a test on a small set of examples to assert that the network predicts similar scores for every class, the loss function without regularization equals $-\log(1/K)$ and adding regularization increases the loss~\cite{Karpathy2016}.

	\item Run a gradient check if backpropagation seems faulty. Gradient checks compare the analytic gradient obtained via backpropagation with a numerical gradient obtained via a finite difference approximation~\cite{Karpathy2016}.

	\item Train the network (without regularization) in 10-40 examples to check that the loss function becomes zero. If the network is unable to overfit a tiny subset of data, flexibilize the model.~\cite{Ng2014}.

	\item During training, the loss function evaluated on training examples decreases monotonically~\footnote{Because of momentum and regularization it could increase slightly before decreasing.}. If not, check for implementation errors, reduce the learning rate or augment momentum~\cite{Karpathy2016}.

	\item Monitor the loss function during training to identify underfitting, characterized by high training loss and high validation loss, and overfiting, characterized by low training loss and high validation loss~\cite{Ng2014}.
\end{itemize}

\paragraph{Image segmentation} We usually require to post-process, upsample and threshold the output of a convolutional network to produce a valid segmentation. 
Given that the network is already trained, using the validation set to choose the best techniques is virtually free.% Here we account for some details about this additional steps.

\begin{itemize}
	\item For big input images, use smaller mini-batches to avoid overloading the GPU memory~\cite{Karpathy2016}.

	\item Set the post-processing technique using the validation set. Conditional random fields are specially effective for refining convolutional network scores~\cite{Chen2015}.

	\item Use bicubic (or bilinear) interpolation for upsampling~\cite{Chen2015}. If the upsampling factor is greater than 16, use a learnable upsampling layer or skip layers~\cite{Long2015}.
% This is called a convolutional transpose 3x3, stride 2, learnable. Learned upsampling.

	\item Set the segmentation threshold using the validation set; this hyperparameter is equivalent to the threshold used in classification.
\end{itemize}

\paragraph{Data augmentation} We reduce overfitting in image data by applying label-preserving transformations to the original images to generate additional examples. Transformations include rotations, translations, horizontal and vertical reflections, crops, zooms and jittering (adding noise to the colors).  The network receives different views of the object and learns invariant features; for instance, if we present images of books at different rotations, we expect it to learn to identify a book on any orientation.
% Dieleman galaxies didn't like jittering or scales and crops
% Lo liked rotation and horizontal flipping

\begin{itemize}
	\item Exploit invariances you expect in the data set, e.g., galaxies are rotation-invariant because in space there is no up or down~\cite{Dieleman2015}.

	\item Be careful to preserve the original label of the image. Applying many transformations may cause the image to lose its meaning. 

	\item Generate the augmented images during training to save storage~\cite{Krizhevsky2012}.

	\item If applying many affine transformations, combine them into a single operation. This is faster and reduces information loss~\cite{Dieleman2015}.

	\item Optionally, use data augmentation at test time: present the network with different versions of the image and average its predictions~\cite{Krizhevsky2012}.
\end{itemize}

% Need more sources for this section. I've read it all around.
\paragraph{Unbalanced data}
In practice, it is common to have very few examples of one class compared to the rest. Managing unbalanced data sets is still an open problem.%We offer advice to deal with unbalanced classes.using standard convolutional networks.
\begin{itemize}
	\item For binary classification with rare positive class, use PRAUC as a performance metric. If the threshold is selected using a validation set, $F_1$ score is also valid~\cite{Davis2006}.

	\item For multiclass classification, use the macro-averaged $F_1$ score, an average of $F_1$ scores per class, with validated thresholds~\cite{Ozgur2005}. A multiclass PRAUC exists but is not used.

	\item If using an appropiate metric is impractical, divide each predicted class probability by the prior probability of the class and renormalize~\cite{Bishop1995}. This rescales predictions to account for the original imbalance in the class distribution.

	\item Avoid oversampling, repeating examples from the rare class, and undersampling, discarding examples from the dominant class, because they do not add any information to the data set.

	\item If rare examples are insufficient for learning, augment them more or balance each mini-batch via stratified sampling; the latter is oversampling.
% Augmentation differs from replication in that it enriches the data set with invariant images but proportion of classes remains unchanged.

%	\item One of the preferred methods to learn with unbalanced data sets is to use a modified loss function which gives a higher weight to errors in the rare class so that during training errors in the rare class will produce higher learning in the network parameters. Specific knowledge of the domain is required to estimate the cost of each class of error.
\end{itemize}

\begin{comment}
% Rewrite this if implementing somehting like this

\paragraph{Current trends}
The research community has been moving towards using convolutional networks composed of convolutional layers only. We offer a couple of guidelines for implementing all convolutional networks.  
\begin{itemize}
	\item Change each pooling layer with a convolutional layer with as many feature maps as the previous layer and filter size $2\times 2$ with stride $2$ for normal pooling or $4\times 4$ with stride $2$ for overlapping pooling~\cite{He2015b}. 
	
	\item Average each feature map in the output of the last convolutional layer to produce a single number per feature map and connect this to the output layer~{He2015b}. This replaces all the fully connected layers (except for the last one) by a single average pooling without parameters~\cite{He2015b}.

	\item To take advantage of the reduction of parameters, increase the depth and number of filters per volume.
	
	\item Increase the amount of feature maps when using convolutional pooling operations.
	
	\item Use 1x1 'bottleneck' filters to reduce the volume depth in 2, then a 3x3 filter for the convolution then a 1x1 to restore the volume depth, produces less parameters and more nonlinearities.

	\item For small data sets pooling layers work as a regularizer because they reduce the number of learnable parameters and replacing them with convolutional layers may not be convenient~\cite{Karpathy2016}.	
\end{itemize}

\paragraph{Transfer learning}
When we have a small data set we could use a pretrained convolutional network either as a feature extractor for the new examples or to provide initializations for the new convolutional network, this is called transfer learning. We offer some tips for using a pretrained model specifically for mammographic images.

\begin{itemize}
	\item Using a convolutional network pretrained in natural images, such as the ImageNet database, CIFAR-10, CIFAR-100, etc., may not work for mammographic images because features useful for one kind of classification are not very useful for the other. Nonetheless, given that features become more specific at higher layers, we could discard the higher layers of the network and use only the cropped network~\cite{Karpathy2016}.
 
	\item Depending on the amount of data that we have we could: (1) add some fully connected layers on top of the pretrained network and train only these new layers, (2) add some convoutional and fully connected layers and train these new layers or (3) add convolutional and/or fully connected layers and train the entire network~\cite{Karpathy2016}.

	\item When training on a pretrained model or fine-tuning use an smaller learning rate than when training a network from scratch. Using a small learning rate assures that we do not disturb very much the already good network parameters.~\cite{Karpathy2016}.
\end{itemize}
\end{comment}

\paragraph{Software} We shortly describe four of the most popular packages for deep learning. All have similar capabilities and are available to the public (open-source).

\begin{itemize}
	\item Tensorflow~\cite{Abadi2015} is a Python/C++ library released by Google that supports automatic differentiation on data flow graphs---neural networks, included---, distributed training on clusters of CPUs and GPUs, graph visualization and easy deployment in multiple platforms. It has been quickly adopted by the deep learning community.
	\item Theano~\cite{Bergstra2010, Bastien2012}: Theano is a Python library developed in Python/CUDA at the University of Montreal that performs symbolic differentiation on computational graphs. It is tightly integrated with NumPy and uses the GPU to evaluate mathematical expressions involving multidimensional arrays.
	\item Torch~\cite{Collobert2011}: Torch is a scientific computing framework developed in C/Lua/CUDA at the IDIAP Research Institute. It offers multidimensional arrays (tensors), automatic differentiation, a command line and Lua interface, GPU support and easy building of complex neural network architectures.
	\item Caffe~\cite{Jia2014}: Caffe is a deep learning framework developed in C++/CUDA by the Berkeley Vision and Learning Center (BVLC) and community contributors. It offers a Python, Matlab and command line interface; reference models and tutorials; and fast code with easy GPU activation.
%	\item Keras is a high-level wrapper which uses Theano or Tensorflow as back-end and lets you define graphs and standard deep learning models without getting into low-level details.
\end{itemize}

\bigskip

We acknowledge that mammographic data is different from regular image segmentation data: labelling is imperfect, image sizes and ratio change, images are bigger, quality varies, objects of interest are small in relation to the background, data sets are smaller, texture is uniform across the image, among others. Therefore, some of the advice given above may prove counterproductive. Whenever possible, design decisions should be based on data and compiled results.
